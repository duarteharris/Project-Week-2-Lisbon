{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The Zen of Python, by Tim Peters\n",
      "\n",
      "Beautiful is better than ugly.\n",
      "Explicit is better than implicit.\n",
      "Simple is better than complex.\n",
      "Complex is better than complicated.\n",
      "Flat is better than nested.\n",
      "Sparse is better than dense.\n",
      "Readability counts.\n",
      "Special cases aren't special enough to break the rules.\n",
      "Although practicality beats purity.\n",
      "Errors should never pass silently.\n",
      "Unless explicitly silenced.\n",
      "In the face of ambiguity, refuse the temptation to guess.\n",
      "There should be one-- and preferably only one --obvious way to do it.\n",
      "Although that way may not be obvious at first unless you're Dutch.\n",
      "Now is better than never.\n",
      "Although never is often better than *right* now.\n",
      "If the implementation is hard to explain, it's a bad idea.\n",
      "If the implementation is easy to explain, it may be a good idea.\n",
      "Namespaces are one honking great idea -- let's do more of those!\n"
     ]
    }
   ],
   "source": [
    "# imports\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import this"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# creating a dataset list for all datasets\n",
    "files = [file for file in os.listdir('gtfs_2')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading each dataset to each dataframe\n",
    "agency = pd.read_csv('gtfs_2/agency.txt')\n",
    "dates = pd.read_csv('gtfs_2/calendar_dates.txt')\n",
    "stop_times = pd.read_csv('gtfs_2/stop_times.txt')\n",
    "frequencies = pd.read_csv('gtfs_2/frequencies.txt')\n",
    "shapes = pd.read_csv('gtfs_2/shapes.txt')\n",
    "trips = pd.read_csv('gtfs_2/trips.txt')\n",
    "stops = pd.read_csv('gtfs_2/stops.txt')\n",
    "calendar = pd.read_csv('gtfs_2/calendar.txt')\n",
    "routes = pd.read_csv('gtfs_2/routes.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dict name: all_ds. \n",
      "\n",
      "Keys: dict_keys(['agency', 'dates', 'stop times', 'frequencies', 'shapes', 'trips', 'stops', 'calendar', 'routes']), \n",
      "DataSets: 9\n"
     ]
    }
   ],
   "source": [
    "# Creating a dict w/all datasets:\n",
    "all_ds = {\"agency\": agency, \n",
    "          \"dates\": dates, \n",
    "          \"stop times\": stop_times, \n",
    "          \"frequencies\": frequencies, \n",
    "          \"shapes\": shapes, \n",
    "          \"trips\": trips, \n",
    "          \"stops\": stops, \n",
    "          \"calendar\": calendar, \n",
    "          \"routes\": routes}\n",
    "\n",
    "access = f\"Dict name: all_ds. \\n\\nKeys: {all_ds.keys()}, \\nDataSets: {len(all_ds)}\"\n",
    "print(access)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "route_short_name    10\n",
       "route_desc          10\n",
       "route_url           10\n",
       "route_color         10\n",
       "route_text_color    10\n",
       "dtype: int64"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# checking how prevalent missing values are in our data (for each dataset)\n",
    "\n",
    "# defining a function to check null values:\n",
    "def null_cols(ds):\n",
    "    \"\"\"check whether the value in each field is missing (null) and return either \n",
    "    True or False for each field, totaling up the number of True values by column. \"\"\"\n",
    "    return ds.isnull().sum()\n",
    "\n",
    "# applying the filter to each dataset\n",
    "agency_null_cols = null_cols(agency)\n",
    "dates_null_cols = null_cols(dates)\n",
    "stop_times_null_cols = null_cols(stop_times)\n",
    "frequencies_null_cols = null_cols(frequencies)\n",
    "shapes_null_cols = null_cols(shapes)\n",
    "trips_null_cols = null_cols(trips)\n",
    "stops_null_cols = null_cols(stops)\n",
    "calendar_null_cols = null_cols(calendar)\n",
    "routes_null_cols = null_cols(routes)\n",
    "\n",
    "# Adding a condition that will filter the data and show us only columns where the number \n",
    "# of null values were greater than zero for each dataset:\n",
    "\n",
    "# 'agency_phone' = 1\n",
    "agency_null_cols[agency_null_cols > 0] \n",
    "\n",
    "# dates['exception_type'].value_counts() == 1 \n",
    "dates_null_cols[dates_null_cols > 0]  \n",
    "\n",
    "# ['stop_headsign', 'pickup_type', 'drop_off_type', shape_dist_traveled] = 1842 (All entries)\n",
    "stop_times_null_cols[stop_times_null_cols > 0] \n",
    "\n",
    "# frequencies['exact_times'].value_counts() == 0\n",
    "frequencies_null_cols[frequencies_null_cols > 0] \n",
    "\n",
    "# 'shape_dist_traveled' = 182 (All entries)\n",
    "shapes_null_cols[shapes_null_cols > 0] \n",
    "\n",
    "# ['trip_headsign', 'direction_id', 'block_id'] = 132 (All entries)\n",
    "trips_null_cols[trips_null_cols > 0]\n",
    "\n",
    "# ['stop_code', 'stop_desc', 'zone_id', 'stop_url', 'location_type', 'parent_station'] = 49 (All entries)\n",
    "stops_null_cols[stops_null_cols > 0]\n",
    "\n",
    "# nothing to declare\n",
    "calendar_null_cols[calendar_null_cols > 0]\n",
    "\n",
    "# ['route_short_name', 'route_desc', 'route_url', 'route_color', 'route_text_color'] = 10 (All entries)\n",
    "routes_null_cols[routes_null_cols > 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Judgement call: droping information that we don't think it's going to be very useful \n",
    "# to our analysis (removing those columns from your datasets) with the drop method.\n",
    "# We will add these column names to a list, and then we will pass those columns to the \n",
    "# drop method and indicate that we want columns (not rows) dropped by setting the axis \n",
    "# parameter to 1.\n",
    "\n",
    "# defining a function to create a list:\n",
    "def drop_cols(bad_cols):\n",
    "    \"\"\"Adding col names to a list to be droped; in this case as long as the col has a \n",
    "    single null value in it, since, in this case, if it has one, their all null.\"\"\"\n",
    "    return list(bad_cols[bad_cols > 0].index)\n",
    "\n",
    "# TODO: Recheck dates, dates_drop_cols (and all other that have 'dates' in the name),\n",
    "# as well as calendar, for I made a mistake. I rechecked it, but another pass would be good.\n",
    "\n",
    "# applying the function to each ds\n",
    "agency_drop_cols = drop_cols(agency_null_cols)\n",
    "dates_drop_cols = drop_cols(dates_null_cols) # this one has no cols to drop\n",
    "stop_times_drop_cols = drop_cols(stop_times_null_cols)\n",
    "frequencies_drop_cols = drop_cols(frequencies_null_cols) # this one has no cols to drop\n",
    "shapes_drop_cols = drop_cols(shapes_null_cols)\n",
    "trips_drop_cols = drop_cols(trips_null_cols)\n",
    "stops_drop_cols = drop_cols(stops_null_cols)\n",
    "calendar_drop_cols = drop_cols(calendar_null_cols) # this one has no cols to drop\n",
    "routes_drop_cols = drop_cols(routes_null_cols)\n",
    "\n",
    "# Passing those columns to the drop method and indicate that we want columns (not rows) \n",
    "# dropped by setting the axis parameter to 1:\n",
    "agency = agency.drop(agency_drop_cols, axis = 1)\n",
    "dates = dates.drop(dates_drop_cols, axis = 1)\n",
    "stop_times = stop_times.drop(stop_times_drop_cols, axis = 1)\n",
    "frequencies = frequencies.drop(frequencies_drop_cols, axis = 1)\n",
    "shapes = shapes.drop(shapes_drop_cols, axis = 1)\n",
    "trips = trips.drop(trips_drop_cols, axis = 1)\n",
    "stops = stops.drop(stops_drop_cols, axis = 1)\n",
    "calendar = calendar.drop(calendar_drop_cols, axis = 1)\n",
    "routes = routes.drop(routes_drop_cols, axis = 1)\n",
    "\n",
    "# this should've left us w/no cols with null values in the datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dict name: all_ds. \n",
      "\n",
      "Keys: dict_keys(['agency', 'dates', 'stop times', 'frequencies', 'shapes', 'trips', 'stops', 'calendar', 'routes']), \n",
      "DataSets: 9\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>agency_id</th>\n",
       "      <th>route_id</th>\n",
       "      <th>route_long_name</th>\n",
       "      <th>route_type</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>50953</td>\n",
       "      <td>Verde - CAIS DO SODRE</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>50954</td>\n",
       "      <td>Verde - TELHEIRAS</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>170</td>\n",
       "      <td>Vermelha - AEROPORTO</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>4922</td>\n",
       "      <td>Amarela - RATO</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "      <td>164</td>\n",
       "      <td>Amarela - ODIVELAS</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>2</td>\n",
       "      <td>165</td>\n",
       "      <td>Amarela - RATO</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>2</td>\n",
       "      <td>166</td>\n",
       "      <td>Azul - STª APOLÓNIA</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>2</td>\n",
       "      <td>167</td>\n",
       "      <td>Azul - REBOLEIRA</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>2</td>\n",
       "      <td>171</td>\n",
       "      <td>Vermelha - S. SEBASTIÃO</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>2</td>\n",
       "      <td>4921</td>\n",
       "      <td>Amarela - ODIVELAS</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   agency_id  route_id          route_long_name  route_type\n",
       "0          2     50953    Verde - CAIS DO SODRE           1\n",
       "1          2     50954        Verde - TELHEIRAS           1\n",
       "2          2       170     Vermelha - AEROPORTO           1\n",
       "3          2      4922           Amarela - RATO           1\n",
       "4          2       164       Amarela - ODIVELAS           1\n",
       "5          2       165           Amarela - RATO           1\n",
       "6          2       166      Azul - STª APOLÓNIA           1\n",
       "7          2       167         Azul - REBOLEIRA           1\n",
       "8          2       171  Vermelha - S. SEBASTIÃO           1\n",
       "9          2      4921       Amarela - ODIVELAS           1"
      ]
     },
     "execution_count": 97,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(access)\n",
    "# checking Incorrect Values in our data (for each dataset)\n",
    "# The dataset was uploaded at 01/01/2020\n",
    "\n",
    "# all the dates in the 'start_date' pertain to 16/09/2019\n",
    "calendar['start_date'].value_counts()\n",
    "\n",
    "# all the dates in the 'end_date' pertain to 16/09/2029; what does this mean?\n",
    "calendar['end_date'].value_counts()\n",
    "\n",
    "# we have values for dates that are for dates in 2020 that haven't occured yet. Why?\n",
    "# What does 'exception_type' stand for? And why are all entries in it == 1?\n",
    "dates['date'].value_counts()\n",
    "dates['exception_type'].value_counts()\n",
    "\n",
    "# varies between 18 and 8: yellow == 13, green == 13, red == 12, blue == 16, so \n",
    "# I don't think it corresponds to stops in the lines\n",
    "stop_times['trip_id'].value_counts()\n",
    "\n",
    "# varies between 24 and 1\n",
    "stop_times['arrival_time'].value_counts()\n",
    "stop_times['departure_time'].value_counts()\n",
    "\n",
    "# varies between 74 and 26\n",
    "stop_times['stop_id'].value_counts()\n",
    "\n",
    "# varies between 132 and 40\n",
    "stop_times['stop_sequence'].value_counts()\n",
    "\n",
    "# all 'trip_id's == 1\n",
    "frequencies['trip_id'].value_counts()\n",
    "\n",
    "# varies between 14 for 485 and 2 for several\n",
    "frequencies['headway_secs'].value_counts()\n",
    "\n",
    "# all == 0\n",
    "frequencies['exact_times'].value_counts()\n",
    "\n",
    "# between 18 and 8\n",
    "shapes['shape_id'].value_counts()\n",
    "\n",
    "# interesting... look at 'route_id' w/ 'shape_id'; where's 167 in shape_id? \n",
    "# 167 exists 18 times in shapes['shape_id']\n",
    "[col and trips[col].value_counts() for col in trips]\n",
    "\n",
    "# all entries are unique \n",
    "# (13+13+12+16-6, where -6 are intersections == 48; it has 49 entries?)\n",
    "len(stops['stop_id'].unique())\n",
    "\n",
    "# all (132) unique\n",
    "calendar['service_id'].value_counts()\n",
    "\n",
    "# we can drop 'agency_id' and 'route_type'\n",
    "routes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
