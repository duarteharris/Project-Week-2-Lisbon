{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The Zen of Python, by Tim Peters\n",
      "\n",
      "Beautiful is better than ugly.\n",
      "Explicit is better than implicit.\n",
      "Simple is better than complex.\n",
      "Complex is better than complicated.\n",
      "Flat is better than nested.\n",
      "Sparse is better than dense.\n",
      "Readability counts.\n",
      "Special cases aren't special enough to break the rules.\n",
      "Although practicality beats purity.\n",
      "Errors should never pass silently.\n",
      "Unless explicitly silenced.\n",
      "In the face of ambiguity, refuse the temptation to guess.\n",
      "There should be one-- and preferably only one --obvious way to do it.\n",
      "Although that way may not be obvious at first unless you're Dutch.\n",
      "Now is better than never.\n",
      "Although never is often better than *right* now.\n",
      "If the implementation is hard to explain, it's a bad idea.\n",
      "If the implementation is easy to explain, it may be a good idea.\n",
      "Namespaces are one honking great idea -- let's do more of those!\n"
     ]
    }
   ],
   "source": [
    "# imports\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import this"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# creating a dataset list for all datasets\n",
    "#path = '../datasets/data_for_mvp'\n",
    "files = [file for file in os.listdir('gtfs_2')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading each dataset to each dataframe\n",
    "agency = pd.read_csv('gtfs_2/agency.txt')\n",
    "dates = pd.read_csv('gtfs_2/calendar_dates.txt')\n",
    "stop_times = pd.read_csv('gtfs_2/stop_times.txt')\n",
    "frequencies = pd.read_csv('gtfs_2/frequencies.txt')\n",
    "shapes = pd.read_csv('gtfs_2/shapes.txt')\n",
    "trips = pd.read_csv('gtfs_2/trips.txt')\n",
    "stops = pd.read_csv('gtfs_2/stops.txt')\n",
    "calendar = pd.read_csv('gtfs_2/calendar.txt')\n",
    "routes = pd.read_csv('gtfs_2/routes.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dict name: all_ds. \n",
      "\n",
      "Keys: dict_keys(['agency', 'dates', 'stop times', 'frequencies', 'shapes', 'trips', 'stops', 'calendar', 'routes']), \n",
      "DataSets: 9\n"
     ]
    }
   ],
   "source": [
    "# Creating a dict w/all datasets:\n",
    "all_ds = {\"agency\": agency, \n",
    "          \"dates\": dates, \n",
    "          \"stop times\": stop_times, \n",
    "          \"frequencies\": frequencies, \n",
    "          \"shapes\": shapes, \n",
    "          \"trips\": trips, \n",
    "          \"stops\": stops, \n",
    "          \"calendar\": calendar, \n",
    "          \"routes\": routes}\n",
    "\n",
    "access = f\"Dict name: all_ds. \\n\\nKeys: {all_ds.keys()}, \\nDataSets: {len(all_ds)}\"\n",
    "print(access)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "route_short_name    10\n",
       "route_desc          10\n",
       "route_url           10\n",
       "route_color         10\n",
       "route_text_color    10\n",
       "dtype: int64"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# checking how prevalent missing values are in our data (for each dataset)\n",
    "\n",
    "# defining a function to check null values:\n",
    "def null_cols(ds):\n",
    "    \"\"\"check whether the value in each field is missing (null) and return either \n",
    "    True or False for each field, totaling up the number of True values by column. \"\"\"\n",
    "    return ds.isnull().sum()\n",
    "\n",
    "# applying the filter to each dataset\n",
    "agency_null_cols = null_cols(agency)\n",
    "dates_null_cols = null_cols(dates)\n",
    "stop_times_null_cols = null_cols(stop_times)\n",
    "frequencies_null_cols = null_cols(frequencies)\n",
    "shapes_null_cols = null_cols(shapes)\n",
    "trips_null_cols = null_cols(trips)\n",
    "stops_null_cols = null_cols(stops)\n",
    "calendar_null_cols = null_cols(calendar)\n",
    "routes_null_cols = null_cols(routes)\n",
    "\n",
    "# Adding a condition that will filter the data and show us only columns where the number \n",
    "# of null values were greater than zero for each dataset:\n",
    "\n",
    "# 'agency_phone' = 1\n",
    "agency_null_cols[agency_null_cols > 0] \n",
    "\n",
    "# dates['exception_type'].value_counts() == 1 \n",
    "dates_null_cols[dates_null_cols > 0]  \n",
    "\n",
    "# ['stop_headsign', 'pickup_type', 'drop_off_type', shape_dist_traveled] = 1842 (All entries)\n",
    "stop_times_null_cols[stop_times_null_cols > 0] \n",
    "\n",
    "# frequencies['exact_times'].value_counts() == 0\n",
    "frequencies_null_cols[frequencies_null_cols > 0] \n",
    "\n",
    "# 'shape_dist_traveled' = 182 (All entries)\n",
    "shapes_null_cols[shapes_null_cols > 0] \n",
    "\n",
    "# ['trip_headsign', 'direction_id', 'block_id'] = 132 (All entries)\n",
    "trips_null_cols[trips_null_cols > 0]\n",
    "\n",
    "# ['stop_code', 'stop_desc', 'zone_id', 'stop_url', 'location_type', 'parent_station'] = 49 (All entries)\n",
    "stops_null_cols[stops_null_cols > 0]\n",
    "\n",
    "# nothing to declare\n",
    "calendar_null_cols[calendar_null_cols > 0]\n",
    "\n",
    "# ['route_short_name', 'route_desc', 'route_url', 'route_color', 'route_text_color'] = 10 (All entries)\n",
    "routes_null_cols[routes_null_cols > 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Judgement call: droping information that we don't think it's going to be very useful \n",
    "# to our analysis (removing those columns from your datasets) with the drop method.\n",
    "# We will add these column names to a list, and then we will pass those columns to the \n",
    "# drop method and indicate that we want columns (not rows) dropped by setting the axis \n",
    "# parameter to 1.\n",
    "\n",
    "# defining a function to create a list:\n",
    "def drop_cols(bad_cols):\n",
    "    \"\"\"Adding col names to a list to be droped; in this case as long as the col has a \n",
    "    single null value in it, since, in this case, if it has one, their all null.\"\"\"\n",
    "    return list(bad_cols[bad_cols > 0].index)\n",
    "\n",
    "# TODO: Recheck dates, dates_drop_cols (and all other that have 'dates' in the name),\n",
    "# as well as calendar, for I made a mistake. I rechecked it, but another pass would be good.\n",
    "\n",
    "# applying the function to each ds\n",
    "agency_drop_cols = drop_cols(agency_null_cols)\n",
    "dates_drop_cols = drop_cols(dates_null_cols) # this one has no cols to drop\n",
    "stop_times_drop_cols = drop_cols(stop_times_null_cols)\n",
    "frequencies_drop_cols = drop_cols(frequencies_null_cols) # this one has no cols to drop\n",
    "shapes_drop_cols = drop_cols(shapes_null_cols)\n",
    "trips_drop_cols = drop_cols(trips_null_cols)\n",
    "stops_drop_cols = drop_cols(stops_null_cols)\n",
    "calendar_drop_cols = drop_cols(calendar_null_cols) # this one has no cols to drop\n",
    "routes_drop_cols = drop_cols(routes_null_cols)\n",
    "\n",
    "# Passing those columns to the drop method and indicate that we want columns (not rows) \n",
    "# dropped by setting the axis parameter to 1:\n",
    "agency = agency.drop(agency_drop_cols, axis = 1)\n",
    "dates = dates.drop(dates_drop_cols, axis = 1)\n",
    "stop_times = stop_times.drop(stop_times_drop_cols, axis = 1)\n",
    "frequencies = frequencies.drop(frequencies_drop_cols, axis = 1)\n",
    "shapes = shapes.drop(shapes_drop_cols, axis = 1)\n",
    "trips = trips.drop(trips_drop_cols, axis = 1)\n",
    "stops = stops.drop(stops_drop_cols, axis = 1)\n",
    "calendar = calendar.drop(calendar_drop_cols, axis = 1)\n",
    "routes = routes.drop(routes_drop_cols, axis = 1)\n",
    "\n",
    "# this should've left us w/no cols with null values in the datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dict name: all_ds. \n",
      "\n",
      "Keys: dict_keys(['agency', 'dates', 'stop times', 'frequencies', 'shapes', 'trips', 'stops', 'calendar', 'routes']), \n",
      "DataSets: 9\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "25816319    1\n",
       "25816368    1\n",
       "25816354    1\n",
       "25816355    1\n",
       "25816356    1\n",
       "           ..\n",
       "25816281    1\n",
       "25816282    1\n",
       "25816283    1\n",
       "25816284    1\n",
       "25816320    1\n",
       "Name: service_id, Length: 132, dtype: int64"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(access)\n",
    "\n",
    "# checking Incorrect Values in our data (for each dataset)\n",
    "# The dataset was uploaded at 01/01/2020\n",
    "\n",
    "# all the dates in the 'start_date' pertain to 16/09/2019\n",
    "calendar['start_date'].value_counts()\n",
    "\n",
    "# all the dates in the 'end_date' pertain to 16/09/2029; what does this mean?\n",
    "calendar['end_date'].value_counts()\n",
    "\n",
    "# we have values for dates that are for dates in 2020 that haven't occured yet. Why?\n",
    "# What does 'exception_type' stand for? And why are all entries in it == 1?\n",
    "dates['date'].value_counts()\n",
    "dates['exception_type'].value_counts()\n",
    "\n",
    "# varies between 18 and 8: yellow == 13, green == 13, red == 12, blue == 18, so \n",
    "stop_times['trip_id'].value_counts()\n",
    "\n",
    "# varies between 24 and 1\n",
    "stop_times['arrival_time'].value_counts()\n",
    "stop_times['departure_time'].value_counts()\n",
    "\n",
    "# varies between 74 and 26\n",
    "stop_times['stop_id'].value_counts()\n",
    "\n",
    "# varies between 132 and 40\n",
    "stop_times['stop_sequence'].value_counts()\n",
    "\n",
    "# all 'trip_id's == 1\n",
    "frequencies['trip_id'].value_counts()\n",
    "\n",
    "# varies between 14 for 485 and 2 for several\n",
    "frequencies['headway_secs'].value_counts()\n",
    "\n",
    "# all == 0\n",
    "frequencies['exact_times'].value_counts()\n",
    "\n",
    "# between 18 and 8\n",
    "shapes['shape_id'].value_counts()\n",
    "\n",
    "# interesting... look at 'route_id' w/ 'shape_id'\n",
    "[col and trips[col].value_counts() for col in trips]\n",
    "trips[trips['shape_id'] == 167]\n",
    "trips['shape_id'].value_counts()\n",
    "\n",
    "# all entries are unique \n",
    "# (13+13+12+18-6, where -6 are intersections == 50; it has 49 entries? ARROIOS is missing\n",
    "# from this list)\n",
    "len(stops['stop_id'].unique())\n",
    "\n",
    "# all (132) unique\n",
    "calendar['service_id'].value_counts()\n",
    "\n",
    "# we can drop 'agency_id' and 'route_type'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dict name: all_ds. \n",
      "\n",
      "Keys: dict_keys(['agency', 'dates', 'stop times', 'frequencies', 'shapes', 'trips', 'stops', 'calendar', 'routes']), \n",
      "DataSets: 9\n"
     ]
    }
   ],
   "source": [
    "print(access)\n",
    "# Identifying Low Variance Columns (where the majority of the values in the column are the \n",
    "# same). Since there is not a lot of variability in these columns, they have the potential \n",
    "# to not be as informative as columns that have a variety of different values in them.\n",
    "\n",
    "def low_variance(ds):\n",
    "    \"\"\"Creating a function that iterates over each col, takes the minimum and the 90th \n",
    "    percentile value for all the numeric columns in our data set (identified via the \n",
    "    _get_numeric_data method), and if the 90th percentile and the minimum are equal to each \n",
    "    other, meaning that at least 90% of the values in that column are the same, it will \n",
    "    add that column name to a low_variance list.\"\"\"\n",
    "    \n",
    "    return [col for col \n",
    "            in ds._get_numeric_data() \n",
    "            if min(ds[col]) == np.percentile(ds[col], 90)]\n",
    "\n",
    "# Creating an empty list (low_variance) to hold the names of columns that fit our criteria:\n",
    "agency_low_variance = low_variance(agency) # 'agency_id'\n",
    "dates_low_variance = low_variance(dates) # 'exception_type'\n",
    "stop_times_low_variance = low_variance(stop_times) # none\n",
    "frequencies_low_variance = low_variance(frequencies) # 'exact_times'\n",
    "shapes_low_variance = low_variance(shapes) # none\n",
    "trips_low_variance = low_variance(trips) # none\n",
    "stops_low_variance = low_variance(stops) # none\n",
    "calendar_low_variance = low_variance(calendar) # ['start_date', 'end_date']\n",
    "routes_low_variance = low_variance(routes) # ['agency_id', 'route_type']\n",
    "\n",
    "# Checking the values that do exist in these fields to confirm that they are not very \n",
    "# informative. \n",
    "agency[agency_low_variance]\n",
    "dates['exception_type'].value_counts()\n",
    "frequencies['exact_times'].value_counts()\n",
    "calendar['start_date'].value_counts()\n",
    "calendar['end_date'].value_counts()\n",
    "routes[routes_low_variance]\n",
    "\n",
    "# Using the the drop method to remove those columns (axis = 1) from our data frame.\n",
    "agency = agency.drop(agency_low_variance, axis = 1)\n",
    "dates = dates.drop(dates_low_variance, axis = 1)\n",
    "frequencies = frequencies.drop(frequencies_low_variance, axis = 1)\n",
    "calendar = calendar.drop(calendar_low_variance, axis = 1)\n",
    "routes = routes.drop(routes_low_variance, axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dict name: all_ds. \n",
      "\n",
      "Keys: dict_keys(['agency', 'dates', 'stop times', 'frequencies', 'shapes', 'trips', 'stops', 'calendar', 'routes']), \n",
      "DataSets: 9\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>route_id</th>\n",
       "      <th>route_long_name</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>50953</td>\n",
       "      <td>Verde - CAIS DO SODRE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>50954</td>\n",
       "      <td>Verde - TELHEIRAS</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>170</td>\n",
       "      <td>Vermelha - AEROPORTO</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>4922</td>\n",
       "      <td>Amarela - RATO</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>164</td>\n",
       "      <td>Amarela - ODIVELAS</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>165</td>\n",
       "      <td>Amarela - RATO</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>166</td>\n",
       "      <td>Azul - STª APOLÓNIA</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>167</td>\n",
       "      <td>Azul - REBOLEIRA</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>171</td>\n",
       "      <td>Vermelha - S. SEBASTIÃO</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>4921</td>\n",
       "      <td>Amarela - ODIVELAS</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   route_id          route_long_name\n",
       "0     50953    Verde - CAIS DO SODRE\n",
       "1     50954        Verde - TELHEIRAS\n",
       "2       170     Vermelha - AEROPORTO\n",
       "3      4922           Amarela - RATO\n",
       "4       164       Amarela - ODIVELAS\n",
       "5       165           Amarela - RATO\n",
       "6       166      Azul - STª APOLÓNIA\n",
       "7       167         Azul - REBOLEIRA\n",
       "8       171  Vermelha - S. SEBASTIÃO\n",
       "9      4921       Amarela - ODIVELAS"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(access)\n",
    "\n",
    "# Extreme Values and Outliers\n",
    "# These outliers can influence our aggregations when we are analyzing data later, so we \n",
    "# want to make sure we address them during our data cleaning stage.\n",
    "\n",
    "# Using IQR (Inter Quartile Range) to identify outliers. \n",
    "\n",
    "# Once the IQR is calculated, it is multiplied by a constant (typically 1.5), and lower and \n",
    "# upper bounds are established at:\n",
    "# 25th Percentile - (IQR x 1.5)\n",
    "# 75th Percentile + (IQR x 1.5)\n",
    "\n",
    "# Any values outside this range are potential outliers and should be investigated.\n",
    "# We will use the Pandas describe function to easily calculate the 25th and 75th \n",
    "# percentiles for every column and transpose the results so that we can easily reference \n",
    "# the values in calculating the interquartile ranges.\n",
    "\n",
    "def iqr(ds):\n",
    "    \"\"\"creates a IQR column from a dataset and transposes the results.\"\"\"\n",
    "    return ds.describe().transpose()\n",
    "\n",
    "# Creating the IQR tale for each dataset by applying the iqr function to each dataset.\n",
    "# It doesn't work (and it isn't necessary?) for agency\n",
    "dates_stats = iqr(dates)\n",
    "stop_times_stats = iqr(stop_times)\n",
    "frequencies_stats = iqr(frequencies)\n",
    "shapes_stats = iqr(shapes)\n",
    "trips_stats = iqr(trips)\n",
    "stops_stats = iqr(stops)\n",
    "calendar_stats = iqr(calendar)\n",
    "routes_stats = iqr(routes)\n",
    "\n",
    "# Adding 'IQR' table to the stats of each dataset\n",
    "dates_stats['IQR'] = dates_stats['75%'] - dates_stats['25%']\n",
    "stop_times_stats['IQR'] = stop_times_stats['75%'] - stop_times_stats['25%']\n",
    "frequencies_stats['IQR'] = frequencies_stats['75%'] - frequencies_stats['25%']\n",
    "#_stats['IQR'] = _stats['75%'] - _stats['25%']\n",
    "routes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dict name: all_ds. \n",
      "\n",
      "Keys: dict_keys(['agency', 'dates', 'stop times', 'frequencies', 'shapes', 'trips', 'stops', 'calendar', 'routes']), \n",
      "DataSets: 9\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>count</th>\n",
       "      <th>mean</th>\n",
       "      <th>std</th>\n",
       "      <th>min</th>\n",
       "      <th>25%</th>\n",
       "      <th>50%</th>\n",
       "      <th>75%</th>\n",
       "      <th>max</th>\n",
       "      <th>IQR</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>service_id</td>\n",
       "      <td>510.0</td>\n",
       "      <td>25816336.1</td>\n",
       "      <td>35.328987</td>\n",
       "      <td>25816284.0</td>\n",
       "      <td>25816291.0</td>\n",
       "      <td>25816346.5</td>\n",
       "      <td>25816376.0</td>\n",
       "      <td>25816383.0</td>\n",
       "      <td>85.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>date</td>\n",
       "      <td>510.0</td>\n",
       "      <td>20197901.0</td>\n",
       "      <td>4376.119348</td>\n",
       "      <td>20191005.0</td>\n",
       "      <td>20191225.0</td>\n",
       "      <td>20200404.0</td>\n",
       "      <td>20201005.0</td>\n",
       "      <td>20201225.0</td>\n",
       "      <td>9780.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            count        mean          std         min         25%  \\\n",
       "service_id  510.0  25816336.1    35.328987  25816284.0  25816291.0   \n",
       "date        510.0  20197901.0  4376.119348  20191005.0  20191225.0   \n",
       "\n",
       "                   50%         75%         max     IQR  \n",
       "service_id  25816346.5  25816376.0  25816383.0    85.0  \n",
       "date        20200404.0  20201005.0  20201225.0  9780.0  "
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(access)\n",
    "dates_stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
