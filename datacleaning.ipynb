{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The Zen of Python, by Tim Peters\n",
      "\n",
      "Beautiful is better than ugly.\n",
      "Explicit is better than implicit.\n",
      "Simple is better than complex.\n",
      "Complex is better than complicated.\n",
      "Flat is better than nested.\n",
      "Sparse is better than dense.\n",
      "Readability counts.\n",
      "Special cases aren't special enough to break the rules.\n",
      "Although practicality beats purity.\n",
      "Errors should never pass silently.\n",
      "Unless explicitly silenced.\n",
      "In the face of ambiguity, refuse the temptation to guess.\n",
      "There should be one-- and preferably only one --obvious way to do it.\n",
      "Although that way may not be obvious at first unless you're Dutch.\n",
      "Now is better than never.\n",
      "Although never is often better than *right* now.\n",
      "If the implementation is hard to explain, it's a bad idea.\n",
      "If the implementation is easy to explain, it may be a good idea.\n",
      "Namespaces are one honking great idea -- let's do more of those!\n"
     ]
    }
   ],
   "source": [
    "# imports\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import this"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# creating a dataset list for all datasets\n",
    "#path = '../datasets/data_for_mvp'\n",
    "files = [file for file in os.listdir('gtfs_2')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading each dataset to each dataframe\n",
    "agency = pd.read_csv('gtfs_2/agency.txt')\n",
    "dates = pd.read_csv('gtfs_2/calendar_dates.txt')\n",
    "stop_times = pd.read_csv('gtfs_2/stop_times.txt')\n",
    "frequencies = pd.read_csv('gtfs_2/frequencies.txt')\n",
    "shapes = pd.read_csv('gtfs_2/shapes.txt')\n",
    "trips = pd.read_csv('gtfs_2/trips.txt')\n",
    "stops = pd.read_csv('gtfs_2/stops.txt')\n",
    "calendar = pd.read_csv('gtfs_2/calendar.txt')\n",
    "routes = pd.read_csv('gtfs_2/routes.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dict name: all_ds. \n",
      "\n",
      "Keys: dict_keys(['agency', 'dates', 'stop times', 'frequencies', 'shapes', 'trips', 'stops', 'calendar', 'routes']), \n",
      "DataSets: 9\n"
     ]
    }
   ],
   "source": [
    "# Creating a dict w/all datasets:\n",
    "all_ds = {\"agency\": agency, \n",
    "          \"dates\": dates, \n",
    "          \"stop times\": stop_times, \n",
    "          \"frequencies\": frequencies, \n",
    "          \"shapes\": shapes, \n",
    "          \"trips\": trips, \n",
    "          \"stops\": stops, \n",
    "          \"calendar\": calendar, \n",
    "          \"routes\": routes}\n",
    "\n",
    "access = f\"Dict name: all_ds. \\n\\nKeys: {all_ds.keys()}, \\nDataSets: {len(all_ds)}\"\n",
    "print(access)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "route_short_name    10\n",
       "route_desc          10\n",
       "route_url           10\n",
       "route_color         10\n",
       "route_text_color    10\n",
       "dtype: int64"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# checking how prevalent missing values are in our data (for each dataset)\n",
    "\n",
    "# defining a function to check null values:\n",
    "def null_cols(ds):\n",
    "    \"\"\"check whether the value in each field is missing (null) and return either \n",
    "    True or False for each field, totaling up the number of True values by column. \"\"\"\n",
    "    return ds.isnull().sum()\n",
    "\n",
    "# applying the filter to each dataset\n",
    "agency_null_cols = null_cols(agency)\n",
    "dates_null_cols = null_cols(dates)\n",
    "stop_times_null_cols = null_cols(stop_times)\n",
    "frequencies_null_cols = null_cols(frequencies)\n",
    "shapes_null_cols = null_cols(shapes)\n",
    "trips_null_cols = null_cols(trips)\n",
    "stops_null_cols = null_cols(stops)\n",
    "calendar_null_cols = null_cols(calendar)\n",
    "routes_null_cols = null_cols(routes)\n",
    "\n",
    "# Adding a condition that will filter the data and show us only columns where the number \n",
    "# of null values were greater than zero for each dataset:\n",
    "\n",
    "# 'agency_phone' = 1\n",
    "agency_null_cols[agency_null_cols > 0] \n",
    "\n",
    "# dates['exception_type'].value_counts() == 1 \n",
    "dates_null_cols[dates_null_cols > 0]  \n",
    "\n",
    "# ['stop_headsign', 'pickup_type', 'drop_off_type', shape_dist_traveled] = 1842 (All entries)\n",
    "stop_times_null_cols[stop_times_null_cols > 0] \n",
    "\n",
    "# frequencies['exact_times'].value_counts() == 0\n",
    "frequencies_null_cols[frequencies_null_cols > 0] \n",
    "\n",
    "# 'shape_dist_traveled' = 182 (All entries)\n",
    "shapes_null_cols[shapes_null_cols > 0] \n",
    "\n",
    "# ['trip_headsign', 'direction_id', 'block_id'] = 132 (All entries)\n",
    "trips_null_cols[trips_null_cols > 0]\n",
    "\n",
    "# ['stop_code', 'stop_desc', 'zone_id', 'stop_url', 'location_type', 'parent_station'] = 49 (All entries)\n",
    "stops_null_cols[stops_null_cols > 0]\n",
    "\n",
    "# nothing to declare\n",
    "calendar_null_cols[calendar_null_cols > 0]\n",
    "\n",
    "# ['route_short_name', 'route_desc', 'route_url', 'route_color', 'route_text_color'] = 10 (All entries)\n",
    "routes_null_cols[routes_null_cols > 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Judgement call: droping information that we don't think it's going to be very useful \n",
    "# to our analysis (removing those columns from your datasets) with the drop method.\n",
    "# We will add these column names to a list, and then we will pass those columns to the \n",
    "# drop method and indicate that we want columns (not rows) dropped by setting the axis \n",
    "# parameter to 1.\n",
    "\n",
    "# defining a function to create a list:\n",
    "def drop_cols(bad_cols):\n",
    "    \"\"\"Adding col names to a list to be droped; in this case as long as the col has a \n",
    "    single null value in it, since, in this case, if it has one, their all null.\"\"\"\n",
    "    return list(bad_cols[bad_cols > 0].index)\n",
    "\n",
    "# TODO: Recheck dates, dates_drop_cols (and all other that have 'dates' in the name),\n",
    "# as well as calendar, for I made a mistake. I rechecked it, but another pass would be good.\n",
    "\n",
    "# applying the function to each ds\n",
    "agency_drop_cols = drop_cols(agency_null_cols)\n",
    "dates_drop_cols = drop_cols(dates_null_cols) # this one has no cols to drop\n",
    "stop_times_drop_cols = drop_cols(stop_times_null_cols)\n",
    "frequencies_drop_cols = drop_cols(frequencies_null_cols) # this one has no cols to drop\n",
    "shapes_drop_cols = drop_cols(shapes_null_cols)\n",
    "trips_drop_cols = drop_cols(trips_null_cols)\n",
    "stops_drop_cols = drop_cols(stops_null_cols)\n",
    "calendar_drop_cols = drop_cols(calendar_null_cols) # this one has no cols to drop\n",
    "routes_drop_cols = drop_cols(routes_null_cols)\n",
    "\n",
    "# Passing those columns to the drop method and indicate that we want columns (not rows) \n",
    "# dropped by setting the axis parameter to 1:\n",
    "agency = agency.drop(agency_drop_cols, axis = 1)\n",
    "dates = dates.drop(dates_drop_cols, axis = 1)\n",
    "stop_times = stop_times.drop(stop_times_drop_cols, axis = 1)\n",
    "frequencies = frequencies.drop(frequencies_drop_cols, axis = 1)\n",
    "shapes = shapes.drop(shapes_drop_cols, axis = 1)\n",
    "trips = trips.drop(trips_drop_cols, axis = 1)\n",
    "stops = stops.drop(stops_drop_cols, axis = 1)\n",
    "calendar = calendar.drop(calendar_drop_cols, axis = 1)\n",
    "routes = routes.drop(routes_drop_cols, axis = 1)\n",
    "\n",
    "# this should've left us w/no cols with null values in the datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dict name: all_ds. \n",
      "\n",
      "Keys: dict_keys(['agency', 'dates', 'stop times', 'frequencies', 'shapes', 'trips', 'stops', 'calendar', 'routes']), \n",
      "DataSets: 9\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "25816319    1\n",
       "25816368    1\n",
       "25816354    1\n",
       "25816355    1\n",
       "25816356    1\n",
       "           ..\n",
       "25816281    1\n",
       "25816282    1\n",
       "25816283    1\n",
       "25816284    1\n",
       "25816320    1\n",
       "Name: service_id, Length: 132, dtype: int64"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(access)\n",
    "\n",
    "# checking Incorrect Values in our data (for each dataset)\n",
    "# The dataset was uploaded at 01/01/2020\n",
    "\n",
    "# all the dates in the 'start_date' pertain to 16/09/2019\n",
    "calendar['start_date'].value_counts()\n",
    "\n",
    "# all the dates in the 'end_date' pertain to 16/09/2029; what does this mean?\n",
    "calendar['end_date'].value_counts()\n",
    "\n",
    "# we have values for dates that are for dates in 2020 that haven't occured yet. Why?\n",
    "# What does 'exception_type' stand for? And why are all entries in it == 1?\n",
    "dates['date'].value_counts()\n",
    "dates['exception_type'].value_counts()\n",
    "\n",
    "# varies between 18 and 8: yellow == 13, green == 13, red == 12, blue == 18, so \n",
    "stop_times['trip_id'].value_counts()\n",
    "\n",
    "# varies between 24 and 1\n",
    "stop_times['arrival_time'].value_counts()\n",
    "stop_times['departure_time'].value_counts()\n",
    "\n",
    "# varies between 74 and 26\n",
    "stop_times['stop_id'].value_counts()\n",
    "\n",
    "# varies between 132 and 40\n",
    "stop_times['stop_sequence'].value_counts()\n",
    "\n",
    "# all 'trip_id's == 1\n",
    "frequencies['trip_id'].value_counts()\n",
    "\n",
    "# varies between 14 for 485 and 2 for several\n",
    "frequencies['headway_secs'].value_counts()\n",
    "\n",
    "# all == 0\n",
    "frequencies['exact_times'].value_counts()\n",
    "\n",
    "# between 18 and 8\n",
    "shapes['shape_id'].value_counts()\n",
    "\n",
    "# interesting... look at 'route_id' w/ 'shape_id'\n",
    "[col and trips[col].value_counts() for col in trips]\n",
    "trips[trips['shape_id'] == 167]\n",
    "trips['shape_id'].value_counts()\n",
    "\n",
    "# all entries are unique \n",
    "# (13+13+12+18-6, where -6 are intersections == 50; it has 49 entries? ARROIOS is missing\n",
    "# from this list)\n",
    "len(stops['stop_id'].unique())\n",
    "\n",
    "# all (132) unique\n",
    "calendar['service_id'].value_counts()\n",
    "\n",
    "# we can drop 'agency_id' and 'route_type'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dict name: all_ds. \n",
      "\n",
      "Keys: dict_keys(['agency', 'dates', 'stop times', 'frequencies', 'shapes', 'trips', 'stops', 'calendar', 'routes']), \n",
      "DataSets: 9\n"
     ]
    }
   ],
   "source": [
    "print(access)\n",
    "# Identifying Low Variance Columns (where the majority of the values in the column are the \n",
    "# same). Since there is not a lot of variability in these columns, they have the potential \n",
    "# to not be as informative as columns that have a variety of different values in them.\n",
    "\n",
    "def low_variance(ds):\n",
    "    \"\"\"Creating a function that iterates over each col, takes the minimum and the 90th \n",
    "    percentile value for all the numeric columns in our data set (identified via the \n",
    "    _get_numeric_data method), and if the 90th percentile and the minimum are equal to each \n",
    "    other, meaning that at least 90% of the values in that column are the same, it will \n",
    "    add that column name to a low_variance list.\"\"\"\n",
    "    \n",
    "    return [col for col \n",
    "            in ds._get_numeric_data() \n",
    "            if min(ds[col]) == np.percentile(ds[col], 90)]\n",
    "\n",
    "# Creating an empty list (low_variance) to hold the names of columns that fit our criteria:\n",
    "agency_low_variance = low_variance(agency) # 'agency_id'\n",
    "dates_low_variance = low_variance(dates) # 'exception_type'\n",
    "stop_times_low_variance = low_variance(stop_times) # none\n",
    "frequencies_low_variance = low_variance(frequencies) # 'exact_times'\n",
    "shapes_low_variance = low_variance(shapes) # none\n",
    "trips_low_variance = low_variance(trips) # none\n",
    "stops_low_variance = low_variance(stops) # none\n",
    "calendar_low_variance = low_variance(calendar) # ['start_date', 'end_date']\n",
    "routes_low_variance = low_variance(routes) # ['agency_id', 'route_type']\n",
    "\n",
    "# Checking the values that do exist in these fields to confirm that they are not very \n",
    "# informative. \n",
    "agency[agency_low_variance]\n",
    "dates['exception_type'].value_counts()\n",
    "frequencies['exact_times'].value_counts()\n",
    "calendar['start_date'].value_counts()\n",
    "calendar['end_date'].value_counts()\n",
    "routes[routes_low_variance]\n",
    "\n",
    "# Using the the drop method to remove those columns (axis = 1) from our data frame.\n",
    "agency = agency.drop(agency_low_variance, axis = 1)\n",
    "dates = dates.drop(dates_low_variance, axis = 1)\n",
    "frequencies = frequencies.drop(frequencies_low_variance, axis = 1)\n",
    "#ca"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dict name: all_ds. \n",
      "\n",
      "Keys: dict_keys(['agency', 'dates', 'stop times', 'frequencies', 'shapes', 'trips', 'stops', 'calendar', 'routes']), \n",
      "DataSets: 9\n"
     ]
    }
   ],
   "source": [
    "print(access)\n",
    "\n",
    "# Extreme Values and Outliers\n",
    "# These outliers can influence our aggregations when we are analyzing data later, so we \n",
    "# want to make sure we address them during our data cleaning stage.\n",
    "\n",
    "# Using IQR (Inter Quartile Range) to identify outliers. \n",
    "\n",
    "# Once the IQR is calculated, it is multiplied by a constant (typically 1.5), and lower and \n",
    "# upper bounds are established at:\n",
    "# 25th Percentile - (IQR x 1.5)\n",
    "# 75th Percentile + (IQR x 1.5)\n",
    "\n",
    "# Any values outside this range are potential outliers and should be investigated.\n",
    "# We will use the Pandas describe function to easily calculate the 25th and 75th \n",
    "# percentiles for every column and transpose the results so that we can easily reference \n",
    "# the values in calculating the interquartile ranges.\n",
    "\n",
    "def iqr(ds):\n",
    "    \"\"\"creates a IQR column from a dataset and transposes the results.\"\"\"\n",
    "    return ds.describe().transpose()\n",
    "\n",
    "# Creating the IQR tale for each dataset by applying the iqr function to each dataset.\n",
    "# It doesn't work (and it isn't necessary?) for agency\n",
    "dates_stats = iqr(dates)\n",
    "stop_times_stats = iqr(stop_times)\n",
    "frequencies_stats = iqr(frequencies)\n",
    "shapes_stats = iqr(shapes)\n",
    "trips_stats = iqr(trips)\n",
    "stops_stats = iqr(stops)\n",
    "calendar_stats = iqr(calendar)\n",
    "routes_stats = iqr(routes)\n",
    "\n",
    "# Adding 'IQR' table to the stats of each dataset\n",
    "dates_stats['IQR'] = dates_stats['75%'] - dates_stats['25%']\n",
    "stop_times_stats['IQR'] = stop_times_stats['75%'] - stop_times_stats['25%']\n",
    "frequencies_stats['IQR'] = frequencies_stats['75%'] - frequencies_stats['25%']\n",
    "shapes_stats['IQR'] = shapes_stats['75%'] - shapes_stats['25%']\n",
    "trips_stats['IQR'] = trips_stats['75%'] - trips_stats['25%']\n",
    "stops_stats['IQR'] = stops_stats['75%'] - stops_stats['25%']\n",
    "calendar_stats['IQR'] = calendar_stats['75%'] - calendar_stats['25%']\n",
    "routes_stats['IQR'] = routes_stats['75%'] - routes_stats['25%']\n",
    "\n",
    "# Create an empty data frame called outliers with the same columns as our data set. \n",
    "# Finally, we will loop through each column in the data calculating the lower and upper \n",
    "# bounds, retrieving records where the value for that column falls outside the bounds we \n",
    "# established, and appending those results to our outlier data frame.\n",
    "\n",
    "outliers = pd.DataFrame(columns=data.columns)\n",
    "\n",
    "for col in stats.index:\n",
    "    iqr = stats.at[col,'IQR']\n",
    "    cutoff = iqr * 1.5\n",
    "    lower = stats.at[col,'25%'] - cutoff\n",
    "    upper = stats.at[col,'75%'] + cutoff\n",
    "    results = data[(data[col] < lower) | \n",
    "                   (data[col] > upper)].copy()\n",
    "    results['Outlier'] = col\n",
    "    outliers = outliers.append(results)\n",
    "\n",
    "Our outliers data frame should now be populated with records that you can investigate further and determine whether they should be kept in the data or dropped. The Outlier column we added before appending the results for the column to the outliers data frame will let you know what column in each record contained the outlier. If you find that this method is returning too many results, you can be more stringent with your cutoff criteria (e.g. increasing the constant by which you multiply the IQR to 3 instead of 1.5).\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dict name: all_ds. \n",
      "\n",
      "Keys: dict_keys(['agency', 'dates', 'stop times', 'frequencies', 'shapes', 'trips', 'stops', 'calendar', 'routes']), \n",
      "DataSets: 9\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>count</th>\n",
       "      <th>mean</th>\n",
       "      <th>std</th>\n",
       "      <th>min</th>\n",
       "      <th>25%</th>\n",
       "      <th>50%</th>\n",
       "      <th>75%</th>\n",
       "      <th>max</th>\n",
       "      <th>IQR</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>agency_id</td>\n",
       "      <td>10.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.00</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.00</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>route_id</td>\n",
       "      <td>10.0</td>\n",
       "      <td>11275.3</td>\n",
       "      <td>21002.128083</td>\n",
       "      <td>164.0</td>\n",
       "      <td>166.25</td>\n",
       "      <td>170.5</td>\n",
       "      <td>4921.75</td>\n",
       "      <td>50954.0</td>\n",
       "      <td>4755.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>route_type</td>\n",
       "      <td>10.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.00</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.00</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            count     mean           std    min     25%    50%      75%  \\\n",
       "agency_id    10.0      2.0      0.000000    2.0    2.00    2.0     2.00   \n",
       "route_id     10.0  11275.3  21002.128083  164.0  166.25  170.5  4921.75   \n",
       "route_type   10.0      1.0      0.000000    1.0    1.00    1.0     1.00   \n",
       "\n",
       "                max     IQR  \n",
       "agency_id       2.0     0.0  \n",
       "route_id    50954.0  4755.5  \n",
       "route_type      1.0     0.0  "
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(access)\n",
    "routes_stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "frequencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sec_to_min(secs):\n",
    "    minu = int(secs / 60)\n",
    "    seconds = (secs % 60)\n",
    "    #seconds = int(secs % 60)\n",
    "    return f\"{minu}:{seconds}\"\n",
    "\n",
    "#frequencies['headway_min'] = sec_to_min(frequencies['headway_secs'])\n",
    "\n",
    "for elemento in frequencies['headway_secs']:\n",
    "    \n",
    "    row_in = sec_to_min(elemento)\n",
    "   # print(row_in)\n",
    "    \n",
    "frequencies['headway_min'] = [sec_to_min(elemento) for elemento in frequencies['headway_secs']]\n",
    "frequencies\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(access)\n",
    "\n",
    "# 164 e 4921\n",
    "# 165 e 4922\n",
    "shapes['shape_id'].unique()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 230,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[26100,\n",
       " 34200,\n",
       " 36000,\n",
       " 60000,\n",
       " 68400,\n",
       " 70200,\n",
       " 72000,\n",
       " 75600,\n",
       " 90300,\n",
       " 43200,\n",
       " 90300,\n",
       " 43200,\n",
       " 90300,\n",
       " 3900,\n",
       " 75600,\n",
       " 72000,\n",
       " 70200,\n",
       " 68400,\n",
       " 60000,\n",
       " 36000,\n",
       " 34200,\n",
       " 26100,\n",
       " 43200,\n",
       " 3900,\n",
       " 43200,\n",
       " 3900,\n",
       " 25200,\n",
       " 27000,\n",
       " 34200,\n",
       " 36000,\n",
       " 59400,\n",
       " 70200,\n",
       " 72000,\n",
       " 73800,\n",
       " 75600,\n",
       " 77400,\n",
       " 81000,\n",
       " 90300,\n",
       " 43200,\n",
       " 73800,\n",
       " 81000,\n",
       " 90300,\n",
       " 43200,\n",
       " 73800,\n",
       " 81000,\n",
       " 90300,\n",
       " 3900,\n",
       " 81000,\n",
       " 77400,\n",
       " 75600,\n",
       " 73800,\n",
       " 72000,\n",
       " 70200,\n",
       " 59400,\n",
       " 36000,\n",
       " 34200,\n",
       " 27000,\n",
       " 25200,\n",
       " 3900,\n",
       " 81000,\n",
       " 73800,\n",
       " 43200,\n",
       " 3900,\n",
       " 81000,\n",
       " 73800,\n",
       " 43200,\n",
       " 26100,\n",
       " 35100,\n",
       " 59400,\n",
       " 70200,\n",
       " 73800,\n",
       " 81000,\n",
       " 90300,\n",
       " 43200,\n",
       " 73800,\n",
       " 80100,\n",
       " 90300,\n",
       " 43200,\n",
       " 73800,\n",
       " 80100,\n",
       " 90300,\n",
       " 3900,\n",
       " 81000,\n",
       " 73800,\n",
       " 70200,\n",
       " 59400,\n",
       " 35100,\n",
       " 26100,\n",
       " 43200,\n",
       " 80100,\n",
       " 73800,\n",
       " 3900,\n",
       " 43200,\n",
       " 80100,\n",
       " 73800,\n",
       " 3900,\n",
       " 59400,\n",
       " 59400,\n",
       " 26100,\n",
       " 36000,\n",
       " 58500,\n",
       " 73800,\n",
       " 75600,\n",
       " 80100,\n",
       " 90900,\n",
       " 43200,\n",
       " 73800,\n",
       " 77400,\n",
       " 81000,\n",
       " 90300,\n",
       " 43200,\n",
       " 73800,\n",
       " 77400,\n",
       " 81000,\n",
       " 90300,\n",
       " 4500,\n",
       " 80100,\n",
       " 75600,\n",
       " 73800,\n",
       " 58500,\n",
       " 36000,\n",
       " 26100,\n",
       " 3900,\n",
       " 81000,\n",
       " 77400,\n",
       " 73800,\n",
       " 43200,\n",
       " 43200,\n",
       " 77400,\n",
       " 73800,\n",
       " 3900,\n",
       " 81000]"
      ]
     },
     "execution_count": 230,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# trip_id_total_time = int(end_time - start_time) in secs\n",
    "\n",
    "def treat_trip_times(time):\n",
    "    \n",
    "    split = time.split(\":\", 2)[:3]\n",
    "    \n",
    "    converted_to_secs = (int(split[0]) * 60 * 60) + (int(split[1]) * 60) + (int(split[2]))\n",
    "    \n",
    "    return converted_to_secs\n",
    "\n",
    "\n",
    "#treat_trip_times(frequencies['end_time'][1]) - treat_trip_times(frequencies['start_time'][1])\n",
    "end = list(map(treat_trip_times, frequencies['end_time']))\n",
    "start = list(map(treat_trip_times, frequencies['start_time']))\n",
    "\n",
    "#frequencies['trip_total_time'] \n",
    "\n",
    "# type(treat_trip_times(\"10:00:00\"))\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# iterate start_time\n",
    "# remove \"00\" after second \":\"\n",
    "# transform in int\n",
    "# covert to seconds\n",
    "\n",
    "# repeat the above proccess for end_time\n",
    "\n",
    "# subtract start_time from end_time and assing to a new col called 'trip_id_total_time'\n",
    "\n",
    "\n",
    "\n",
    "# df['DataFrame Column'] = pd.to_numeric(df['DataFrame Column'])\n",
    "#frequencies['trip_id_total_time'] = \n",
    "\"\"\"frequencies['end_time'] = pd.to_numeric(frequencies['end_time'])\n",
    "frequencies['start_time'] = pd.to_numeric(frequencies['start_time'])\"\"\"\n",
    "#frequencies['trip_id_total_time']\n",
    "\n",
    "#frequencies['trip_id_total_time'] = [(int(x[0]))*60*60 + int(x[1])*60 + int(x[2]) for x in l]\n",
    "#frequencies\n",
    "\n",
    "#[(int(x[0]))*60*60 + int(x[1])*60 + int(x[2]) for x in l]\n",
    "\n",
    "\n",
    "   \"\"\" split_start = [trip_time.split(\":\", 2)[:2]\n",
    "                   for trip_time \n",
    "                   in start]\n",
    "    \n",
    "    split_end = [trip_time.split(\":\", 2)[:2]\n",
    "                   for trip_time \n",
    "                   in end]\n",
    "    \n",
    "    start_secs = [int(val) for sublist in split_start for val in sublist]\n",
    "    \n",
    "    end_secs = [int(val) for sublist in split_end for val in sublist]\n",
    "    \"\"\"\n",
    "    \"\"\"valor_querido = [(end_secs[val]*60*60 + end_secs[val]*60 + end_secs[val]) - \n",
    "                     (start_secs[val]*60*60 + start_secs[val]*60 + start_secs[val]) \n",
    "                     for sublist in start for val in sublist]\"\"\"\n",
    "    \n",
    "    #start_secs = [(int(split_start[i][i]) * 60 + int(split_start[i][j])) for i in split_start]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
